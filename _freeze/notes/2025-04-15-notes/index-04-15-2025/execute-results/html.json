{
  "hash": "23b4b03ce19cba49d8d13727e1e0f561",
  "result": {
    "engine": "knitr",
    "markdown": "---\nauthor: DTK\ndate: 2025-04-15\nnumber-offset: 35\nformat: live-html\nwebr:\n  channel-type: 'automatic'\n  repos: [\"https://dtkaplan.r-universe.dev\"]\n  packages: ['ggplot2', 'mosaicCalc', \"LSTbook\" ]\nfilters:\n  - webr\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.callout-tip collapse=true}\n## Background software\n```{webr-r}\n#| autorun: true\n#| context: output\n# Drawing vectors and matrices\n\ndraw_matrix <- function(M) {\n  image(1:ncol(M), 1:nrow(M), t(M[nrow(M):1, ]),\n        col = colorspace::diverging_hcl(\n          15, h = c(180, 50), c = 80,\n          l = c(20, 95), power = c(0.7, 1.3)),\n        axes = FALSE,\n        useRaster = TRUE)\n}\n\nsomevecs <- tibble::tribble(\n  ~ rootx, ~ rooty, ~ headx, ~ heady, ~ color, ~ name,\n  0, 0, -2, 4, \"blue\", \"u\",\n  1, -2, 5, -3, \"green\", \"v\",\n  -1, -1, -4, -1, \"orange\", \"w\",\n  1, 0, 4, 3, \"brown\", \"x\",\n  0, -2, 3, -3, \"salmon\", \"y\",\n  0, 0, 4, 3, \"magenta\", \"b1\",\n  0, 0, -3, 2, \"magenta\", \"b2\",\n) |>\n  dplyr::mutate(labelx = (rootx + headx)/2,\n         labely = (2*rooty + heady)/3)\nsolve_for <- function(vecnames) {\n  somevecs |>\n    dplyr::filter(name %in% vecnames) |>\n    gf_segment(rooty + heady ~ rootx + headx,\n               arrow = grid::arrow(length=unit(0.15, \"inches\"), type=\"closed\"),\n               color = ~ color, linewidth=2) |>\n    gf_label(labely ~ labelx, label= ~ name, color = ~ color, size=3) |>\n    gf_refine(scale_color_identity(),\n              scale_y_continuous(limits=c(-5,5),\n                                 breaks=(-5):5),\n              scale_x_continuous(limits=c(-5,5),\n                                 breaks=(-5):5),\n              coord_fixed()) |>\n    gf_labs(x=\"\", y = \"\")\n}\n\n\n# Simple vector/matrix operations\n\nvec <- function(...) {\n  vals <- list(...) |> unlist()\n  matrix(vals, ncol=1)\n}\n\nrvec <- function(first, ..., rfun=rnorm) {\n  dots <- c(first, list(...)) |> unlist()\n  if (length(dots) == 1) {\n    dots <- rfun(dots[[1]])\n  }\n\n  vec(dots)\n}\n\nveclen <- function(v) sqrt(v %dot% v)\nunitvec <- function(vec) vec/length(vec)\ncang <- function(v, w) {(v %dot% w) / (veclen(v) * veclen(w))}\nmakeQ <- function(...) {\n  dots <- list(...) |> bind_cols()\n  qr.Q(qr(dots))\n}\n\ncheck_for_svd <- function(S) {\n  if (inherits(S, \"matrix\")) S <- svd(S)\n  if (!is.list(S) || !all(c(\"d\",\"v\", \"u\") %in% names(S)))\n    stop(\"Argument must be matrix or the SVD of a matrix.\")\n  S\n}\n\nrank_mat <- function(S, thresh = 0.01) {\n  S <- check_for_svd(S)\n  sum(S$d > S$d[1] * thresh)\n}\n\nrand_mat <- function(nrow=3, ncol=6, rank = pmin(nrow, ncol)) {\n  M <- matrix(runif(nrow*ncol), nrow = nrow, ncol = ncol)\n  if (rank >= nrow || rank >= ncol) {\n   M\n  } else {\n    approx_mat(S, n=sample(pmin(rank, nrow, ncol)))\n  }\n}\n\n# Grab rank 1 matrix from SVD\napprox_mat <- function(S, n=1, order = 0) { # input a matrix or the SVD of a matrix\n  S <- check_for_svd(S)\n  if (order > 0) {\n    inds1 <- order(c(S$u[,order]))\n    inds2 <- order(c(S$v[,order]))\n  } else {\n    inds1 <- 1:nrow(S$u)\n    inds2 <- 1:nrow(S$v)\n  }\n  partial <- 0 # initial value\n  for (k in n) {\n    partial <- partial +  S$d[k] * S$u[inds1,k, drop = FALSE] %*% t(S$v[inds2,k, drop = FALSE])\n  }\n\n  partial\n}\n\npretty_mat <- function(M) {\n  inds1 <- order(rowSums(M))\n  inds2 <- order(colSums(M))\n  M[inds1, inds2]\n}\n\n#' Generate a matrix whose elements are selected randomly from a set\n#' of specified values.\n#' @param nrow number of rows for the matrix produced\n#' @param ncol number of columns\n#' @param values the set from which to draw (randomly) the\n#' values in the matrix. Default: integers -9 to 9\nvalues_mat <- function(nrow=4, ncol=3, values = -9:9) {\n  matrix(sample(values, size = nrow * ncol, replace = TRUE),\n         nrow = nrow, ncol=ncol)\n}\n\n# Typeset matrices in LaTeX (for assignments and such)\nLmat <- function(nr, nc) {\n  values_mat(nr, nc) |> latex_helper()\n}\nlatex_helper <- function(matr) {\n  printmrow <- function(x) {\n\n    cat(cat(x,sep=\" & \"),\"\\\\\\\\ \\n\")\n  }\n\n  cat(\"\\\\left(\\\\begin{array}{r}\",\"\\n\")\n  body <- apply(matr,1,printmrow)\n  cat(\"\\\\end{array}\\\\right)\")\n}\n\n\n```\n:::\n\n\n# 2025-04-15 class notes\n\n```{webr-r}\n#| caption: User console A\n#| persist: true\n```\n\n```{webr-r}\n#| caption: User console B\n#| persist: true\n```\n\n```{webr-r}\n#| caption: User console C\n#| persist: true\n```\n\n\nCreate a matrix whose elements contain the row-numbers.\n\n\n```{webr-r}\n#| autorun: true\nrand_R_mat <- function(m = 4, nonsingular=FALSE) {\n  R <- values_mat(m, m) * \n       (col_ind_mat(m, m) >= row_ind_mat(m, m))\n  if (nonsingular) {\n     D <- diag(R)\n     D[D==0] <- sample(setdiff(-9:9, 0),     \n                       size=sum(D==0),\n                       replace=TRUE)\n     diag(R) <- D\n  }\n\n  R\n\n}\ncol_ind_mat <- function(nrow=4, ncol=4) {\n  matrix(rep(1:ncol, times = nrow), nrow=nrow, byrow=TRUE)\n}\nrow_ind_mat <- function(nrow=4, ncol=4){\n  matrix(rep(1:nrow, times = ncol), nrow=nrow, byrow=FALSE)\n}\n```\n\n\nSolving the matrix equation ${\\bf M}\\ \\vec{\\bf x} = \\vec{\\bf b}$\n\n## Easiest case: ${\\bf M}$ is diagonal\n\n$$\\left(\\begin{array}{webr-r} \n3 & 0 & 0 & 0 & 0 \\\\ \n0 & 1 & 0 & 0 & 0 \\\\ \n0 & 0 & -4 & 0 & 0 \\\\ \n0 & 0 & 0 & 2 & 0 \\\\ \n0 & 0 & 0 & 0 & 7 \\\\ \n\\end{array}\\right)  \\left(\\begin{array}{r} \nx_1 \\\\ \nx_2 \\\\ \nx_3 \\\\ \nx_4 \\\\ \nx_5 \\\\ \n\\end{array}\\right)\\ = \\ \n\\left(\\begin{array}{r} \n6 \\\\ \n-5 \\\\ \n-7 \\\\ \n2 \\\\ \n-5 \\\\ \n\\end{array}\\right)$$\n\nExample in R:\n\n```{webr-r}\nn <- 6\nM <- diag(rnorm(6))\nmade_up_x <- values_mat(nrow = n, ncol = 1)\ncorresponding_b <- M %*% made_up_x\ncbind(made_up_x, 0, corresponding_b)\n```\n\nNow try to solve:\n```{webr-r}\ncorresponding_b / diag(M)\n```\n\n\n  \n## Still easy: upper triangular matrix.\nAn upper triangular matrix `M` (which we'll call `R`, short for \"right triangular\", which is the same as upper triangular).\n\n$$\\left(\\begin{array}{r} \n-8 & -2 & 3 & -6 & 2 \\\\ \n0 & -1 & 1 & -6 & -6 \\\\ \n0 & 0 & 2 & -9 & -2 \\\\ \n0 & 0 & 0 & -1 & -5 \\\\ \n0 & 0 & 0 & 0 & -4 \\\\ \n\\end{array}\\right)  \\left(\\begin{array}{r} \nx_1 \\\\ \nx_2 \\\\ \nx_3 \\\\ \nx_4 \\\\ \nx_5 \\\\ \n\\end{array}\\right)\\ = \\ \n\\left(\\begin{array}{r} \n6 \\\\ \n-5 \\\\ \n-7 \\\\ \n2 \\\\ \n-5 \\\\ \n\\end{array}\\right)$$\n\nTask 1. Proof that there is an $\\vec{\\bf x}$ by which we could reach any possible $\\vec{\\bf b}$.\n\nTask 2. Proof that even if there are zeros above the diagonal, there is still an $\\vec{\\bf x}$ that let's us reach any possible $\\vec{\\bf b}$.\n\nTask 3. Proof that when there are zeros *on the diagonal*, we cannot reach any possible $\\vec{\\bf b}$ but we might be able to reach some $\\vec{\\bf b}$.\n\n## Standard software: `backsolve()`\n\nLet's try the built-in `backsolve()` routine.\n\n```{webr-r}\nn <- 4000\nset.seed(104) # for a singular matrix\nR <- rand_R_mat(n, nonsingular=TRUE)\nb <- R %*% values_mat(n,1)\n#b\n#backsolve(R, b)\nall(R %*% backsolve(R, b) - b  < 0.00000001)\n```\n\nA singular R matrix\n```{webr-r}\nset.seed(104) # for a singular matrix\nR <- rand_R_mat(4)\nb <- R %*% values_mat(4,1)\n```\n\n## Another easy situation: $\\bf M$ has mutually orthogonal vectors.\n\nA small, simple example of a Q matrix:\n\n$${\\bf M} \\equiv \\left(\\begin{array}{r} \n-3 & 2 & 0 & 0  \\\\ \n2 & 3 & 0 & 0 \\\\ \n0 & 0 & 2 & -1\\\\ \n0 & 0 & 1 & 2\n\\end{array}\\right) $$\n\nDemonstrate that it's a Q matrix.\n\nLet's solve a system numerically:\n\n```{webr-r}\nM = cbind(\nvec(-3, 2, 0, 0),\nvec(2, 3, 0, 0),\nvec(0, 0, 2, 1),\nvec(0, 0, -1, 2))\nmade_up_x <- vec(1,1,1,1)\nresulting_b <- M %*% made_up_x\nresulting_b\n```\n\nHow to solve ${\\bf M}\\ \\vec{\\bf x} \\ = \\vec{\\bf b}$ when M is Q?\n\nMultiply both sides by ${\\bf M}^T$.\n\n```{webr-r}\nt(M) %*% M\nqr.solve(t(M) %*% M, t(M) %*% resulting_b)\nx <- t(M) %*% resulting_b / diag(t(M) %*% M)\nM %*% x\n```\n\n\nSo, for Q, R, and diagonal matrices solving the matrix equations is easy. Problems only when R or diagonal has zeros on the diagonal.\n\n## Creating Q out of an M\n\nAlgorithm:\n\n1. Start with a partial Q that is already orthogonal. We'll also make it unit length.\n2. Add a new column to Q which will be the residual from partial Q of the next column of M.\n3. Stop when you have used up all the columns of M.\n\n4. R will be Q^T^ M.\n\nDone\n\n```{webr-r}\nM <- values_mat(5)\nM\nnext_Q_column <- function(partialQ, Mcolumn) {\n  # subtract out projections of Mcolumn\n  # onto each of the columns of partialQ (individually)\n  tmp <- Mcolumn\n  for (k in 1:ncol(partialQ)) {\n    tmp <- tmp - Mcolumn %onto% partialQ[, k, drop=FALSE]\n  }\n  tmp / veclen(tmp)\n}\n```\n```{webr-r}\n## initialize\nfirst_col <- M[, 1, drop = FALSE]\npartial_Q <- first_col / veclen(first_col) \n## loop over remaining columns of M\nfor (k in 2:ncol(M)) {\n  partial_Q <- \n    cbind(partial_Q, next_Q_column(partial_Q, M[,k]))\n}\n# result\nQ <- partial_Q\nR <- t(Q) %*% M\n```\n\nIs QR == M?\n\n```{webr-r}\nQ %*% R\n```\n\nNow we have a three step method.\n\n1. Find a Q corresponding to M \n2. Solve QR = M for R\n3. Pre-multiply $\\vec{\\bf b}$ by Q^T^ from (2).\n4. Backsolve R against result of (3).\n\nPotentially a problem if the R has a zero on the diagonal.\n\n## Experiments\n\n```{webr-r}\nM <- rand_mat(78, 54)\ntmp <- qr(M)\nQ <- qr.Q(tmp)\nR <- qr.R(tmp)\ndiag(R)\n```\n\n## ${\\bf Q}^T$ projects $\\vec{\\bf b}$ onto $\\bf M$ subspace.\n\nLet's make an M and a b\n\n```{webr-r}\nM <- rand_mat(10, 3)\nb <- rand_mat(10, 1)\nx1 <- qr.solve(M, b)\nbhat <- M %*% x1\nresid <- b - bhat\nQ <- M |> qr() |> qr.Q()\nQ\nx2 <- qr.solve(Q, bhat)\nontoQ <- Q %*% x2\nresid_from_Q <- bhat - ontoQ\nveclen(resid_from_Q)\n```\n  \n  \n  \n## Random vectors\n\nA fundament of statistics is the alignment of random vectors:\n\n```{webr-r}\nn <- 40\nRuns <- tibble(cosineang = cang(rvec(n), rvec(n)),\n                ang = cosineang * 180 / pi) |> \n        trials(10000)\nRuns |> summarize(var(cosineang))\nRuns |> point_plot(cosineang ~ 1, annot = \"violin\")\n```\n\n::: {#fig-random-2-and-3}\n![](random-in-2.png)\n![](random-in-3.png)\n:::\n\n## Polynomials as vectors\n\n## Orthogonal functions\n\nConsider the polynomials over the interval $[-1,1]$:\n\n```{webr-r}\nslice_plot(x ~ x, domain(x = -1:1)) |>\n  slice_plot(x^2 ~ x, color = \"orange\") |>\n  slice_plot(x^3 ~ x, color = \"dodgerblue\") |>\n  slice_plot(x^4 ~ x, color = \"tomato\") |>\n  slice_plot(x^5 ~ x, color = \"steelblue\") |>\n  slice_plot(x^6 ~ x, color = \"red\") |>\n  slice_plot(x^7 ~ x, color = \"blue\") \n```\n\n\nMaking a matrix of values:\n\n```{webr-r}\nx <- seq(-1, 1, length = 1001) |> vec()\nM <- cbind(1, x, x^2, x^3, x^4, x^5, x^6, x^7)\ncolnames(M) <- c(\"zeroth\", \"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\")\nsvd(M)$d\n```\n\nOrthogonalize them, plotting out the new functions:\n\n```{webr-r}\nQ <- qr.Q(qr(M))\ncolnames(Q) <- colnames(M)\nQdf <- cbind(x, as.data.frame(Q))\nQdf |>\n  gf_line(zeroth  ~ x) |>\n  gf_line(first   ~ x) |>\n  gf_line(second  ~ x, color = \"orange\") |>\n  gf_line(third   ~ x, color = \"dodgerblue\") |>\n  gf_line(fourth  ~ x, color = \"tomato\") |>\n  gf_line(fifth   ~ x, color = \"steelblue\", linewidth=2) |>\n  gf_line(sixth   ~ x, color = \"red\", linewidth=2) |>\n  gf_line(seventh ~ x, color = \"blue\", linewidth=2) \n```\n\n```{webr-r}\nsvd(M)$d\nsvd(Q)$d\n```\n\n## The geometric theory of confidence intervals\n\n::: {#fig-random-noise-model}\n![](SM-14-5.png)\n:::\n\n\n\n::: {#fig-projection-A-on-BC}\n![](SM-11-2.png)\nProjecting A onto the subspace defined by a set of two model vectors, B and C. The model triangle is shaded.\n:::\n\n::: {#fig-reverse-engineering}\n![](SM-14-6.png)\n\nWe measure response variable A and project onto the model subspace B. The residual gives us an idea of how big the random component is. Since the raw estimate of the residual size is biased to be short, we unbias it by consideration of $n$ and $p$---the dimension of the model subspace.\n:::\n\n::: {#fig-random-angles}\n![](random-angles.png)\n\n\nThe embedding space has dimension $n$. Random vectors in high-dimensional spaces tend to be closer to orthogonal than for low-dimensional spaces.\n:::\n\n::: {#fig-random-vecs-ortho-1}\n#| layout-ncol: 2\n#| fig-subcap:\n#| - Small $n$\n#| - Large $n$\n![](SM-14-7-a.png)\n\n![](SM-14-7-b.png)\n\n:::\n\n::: {#fig-collinearity-CI}\n![](collinearity-CI.png)\nConfidence intervals on the B coefficient for two models. The shaded circle shows the range covered by simulated values of the residuals Asim. The contour lines show the values of the coefficient $c_B$ for any possible value of Asim. The confidence interval can be read off as the range of contours covered by the shaded circle. For the model A ∼ B+C that spacing depends on the angle between B and C.\n:::\n\n$$ \\mbox{standard error of B coef.} = \n|\\mbox{residuals}|\\frac{1}{|\\mbox{B}|}\\ \n\\frac{1}{\\sin( \\theta )}\\ \\frac{1}{\\sqrt{n}}\\ \\sqrt{\\frac{n}{n-m}} .$$\n\n\nThere are five components to this formula, each of which says something about the standard error of a coefficient:\n\ni. |residuals| -- The standard error is directly proportional to the size of the residuals.\n\nii. 1/|B| -- The length of the model vector reflects the units of that variable or, for the indicator vector for a categorical variable, how many cases are at the corresponding level.\n\niii. 1/sin(θ) -- This is the magnifying effect of collinearity.  θ is the angle between explanatory vector B and all the other explanatory vectors.  More precisely, it is the angle between B and the vector   that would be found by fitting B to all the other explanatory vectors.  When θ is 90°, then sin(θ)=1 and there is no collinearity and no magnification.  When θ = 45°, the magnification is only 1.4.  But for strong collinearity, the magnification can be very large: when θ = 10°, the magnification is 5.8.  When θ = 1°, magnification is 57.  For θ = 0, the alignment becomes a matter of redundancy, since B can be exactly modeled by the other explanatory variables.  Magnification would be infinite, but statistical software will identify the redundancy and eliminate it. \n\niv. 1/√n -- This reflects the amount of data.  Larger samples give more precise estimates of coefficients, that is, a smaller standard error.  Because of the square-root relationship, quadrupling the size of the sample will halve the size of the standard error. To improve the standard error by ten-fold requires a 100-fold increase in the size of the sample. \n\nv. $\\sqrt{n/(n-m)} --  The fitting process chooses coefficients that make the residuals as small as possible consistent with the data.  This tends to underestimate the size of residuals – for any other estimate of the coefficients, even one well within the confidence interval, the residuals would be larger. This correction factor balances this bias out.  n is, as always, the number of cases in the sample and m is the number of model vectors.  So long as m is much less than n, which is typical of models, the bias correction factor is close to 1.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}